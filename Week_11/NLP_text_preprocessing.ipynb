{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0f68ae5",
   "metadata": {},
   "source": [
    "# Natural Language Processing - Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2054b14d",
   "metadata": {},
   "source": [
    "## Libraries and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ba4eb3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /workspaces/data_analytics/Week_11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/vscode/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/vscode/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/vscode/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/vscode/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/vscode/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Libraries\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "import nltk\n",
    "\n",
    "# Import only once\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.chunk import tree2conlltags\n",
    "from nltk.chunk import conlltags2tree\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Current working directory\n",
    "print('Current working directory:', os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e252684a",
   "metadata": {},
   "source": [
    "## Defining documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e8057467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A cat is sitting on the window. The dog is running in the park. The birds are flying over the trees.'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining documents (=sentenses)\n",
    "d1 = \"A cat is sitting on the window.\"\n",
    "d2 = \"The dog is running in the park.\"\n",
    "d3 = \"The birds are flying over the trees.\"\n",
    "\n",
    "corpus_01 = d1 + ' ' + d2 + ' ' + d3\n",
    "corpus_01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fadda5",
   "metadata": {},
   "source": [
    "## Text preprocessing\n",
    "#### Steps:\n",
    "- Text to lowercase\n",
    "- Removing punctuations\n",
    "- Tokenization\n",
    "- Removal of stop words\n",
    "- Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e649b8",
   "metadata": {},
   "source": [
    "### Text to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2666c8b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a cat is sitting on the window. the dog is running in the park. the birds are flying over the trees.'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text to lowercase function\n",
    "def text_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "# Text to lowercase\n",
    "corpus_02 = text_lowercase(corpus_01)\n",
    "corpus_02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c837286f",
   "metadata": {},
   "source": [
    "### Removing punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "90067406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a cat is sitting on the window the dog is running in the park the birds are flying over the trees'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove punctuation function\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "# Remove punctuation\n",
    "corpus_03 = remove_punctuation(corpus_02)\n",
    "corpus_03"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986153d2",
   "metadata": {},
   "source": [
    "### Tokenize text & removal of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e2e99fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of english stopwords:\n",
      "{'mightn', 'wouldn', 'such', 'yourselves', 'has', 'too', 'during', 'over', 'the', 'i', 'that', 't', \"hasn't\", 'she', 'below', 'won', 'but', 'with', 'they', 'up', 'having', \"mustn't\", 'by', 'weren', 'yours', 'again', \"shan't\", 'its', \"won't\", 'mustn', 'are', 'until', 'only', 'once', 'myself', 'ain', 'hadn', 'don', 'then', 'where', \"she's\", \"didn't\", 'll', 'through', \"needn't\", \"don't\", 'it', 'if', 'no', 'what', 'his', 're', 'few', 'or', 'and', 'being', 'when', \"hadn't\", 'me', 'how', 'all', \"you've\", 'before', 'been', \"aren't\", 'isn', \"you're\", 'while', 'which', 'didn', 'couldn', 'm', 'hers', \"wouldn't\", 'between', 'off', 'them', 'there', 'now', 'after', \"you'd\", 'nor', 'ours', 'same', 'who', 'herself', 'at', 'into', 'here', 'than', 'this', 'will', 'for', 'her', 'do', \"doesn't\", 'own', 'doesn', 'most', 'y', \"isn't\", 'from', 'should', 'our', 'of', 'my', 'any', 'under', 'd', 've', 'have', 'be', 'down', 'aren', 'shan', 'each', 'was', 's', 'you', \"wasn't\", 'other', 'is', 'why', 'more', 'wasn', \"it's\", 'ourselves', 'had', 'an', 'theirs', 'am', 'about', 'itself', 'doing', 'out', 'haven', 'on', \"that'll\", 'did', 'against', 'their', 'needn', 'shouldn', 'both', \"couldn't\", 'in', 'a', 'some', 'were', 'we', \"haven't\", 'does', 'himself', 'as', 'hasn', \"shouldn't\", 'not', \"should've\", \"mightn't\", 'yourself', 'he', \"you'll\", 'just', 'o', 'your', 'him', 'because', 'very', \"weren't\", 'ma', 'so', 'can', 'those', 'whom', 'themselves', 'these', 'to', 'further', 'above'}\n"
     ]
    }
   ],
   "source": [
    "# Show english stopwords\n",
    "eng_stopwords = set(stopwords.words('english'))\n",
    "print(\"List of english stopwords:\")\n",
    "print(eng_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d83ab939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat', 'sitting', 'window', 'dog', 'running', 'park', 'birds', 'flying', 'trees']"
     ]
    }
   ],
   "source": [
    "# Function for tokenization and the removal of stopwords\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "    return filtered_text\n",
    " \n",
    "# Remove stopwords\n",
    "corpus_04 = remove_stopwords(corpus_03)\n",
    "print(corpus_04, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad590183",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "410fed5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before lemmatization:\n",
      "['cat', 'sitting', 'window', 'dog', 'running', 'park', 'birds', 'flying', 'trees'] \n",
      "\n",
      "After lemmatization:\n",
      "['cat', 'sit', 'window', 'dog', 'run', 'park', 'bird', 'fly', 'tree']"
     ]
    }
   ],
   "source": [
    "# Initialize Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize string function\n",
    "def lemmatize_word(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    lemmas = [lemmatizer.lemmatize(word, pos ='v') for word in word_tokens]\n",
    "    return lemmas\n",
    "\n",
    "# Lemmatize\n",
    "lem = []\n",
    "for i in corpus_04:\n",
    "    lem.append(lemmatize_word(i))\n",
    "\n",
    "# Nested list to list\n",
    "corpus_05 = [' '.join([str(x) for x in lst]) for lst in lem]\n",
    "\n",
    "print('Before lemmatization:')\n",
    "print(corpus_04, '\\n')\n",
    "\n",
    "print('After lemmatization:')\n",
    "print(corpus_05, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9ad6de",
   "metadata": {},
   "source": [
    "## Redefine the text corpus (pre-processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "08a3cb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the lemmatized words above to re-define our corpus \n",
    "corpus = [\n",
    "    'cat sit window',      # From d1: \"A cat is sitting on the window.\"\n",
    "    'dog run park',        # From d2: \"The dog is running in the park.\"\n",
    "    'bird fly tree'        # From d3: \"The birds are flying over the trees.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198cc6d0",
   "metadata": {},
   "source": [
    "## Document-term matrix with ngram_range=(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ead679d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document-term matrix (Unigrams)\n",
      "   bird  cat  dog  fly  park  run  sit  tree  window\n",
      "0     0    1    0    0     0    0    1     0       1\n",
      "1     0    0    1    0     1    1    0     0       0\n",
      "2     1    0    0    1     0    0    0     1       0\n",
      "\n",
      "Document-term matrix (Bigrams)\n",
      "   bird fly  cat sit  dog run  fly tree  run park  sit window\n",
      "0         0        1        0         0         0           1\n",
      "1         0        0        1         0         1           0\n",
      "2         1        0        0         1         0           0\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Define the corpus\n",
    "corpus = [\n",
    "    'cat sit window', \n",
    "    'dog run park', \n",
    "    'bird fly tree'\n",
    "]\n",
    "\n",
    "# Document-term matrix with ngram_range=(1,1) [Unigrams]\n",
    "vectorizer_unigram = CountVectorizer(ngram_range=(1,1), min_df=0.0)\n",
    "\n",
    "# Transform the corpus\n",
    "count_unigram = vectorizer_unigram.fit_transform(corpus)\n",
    "\n",
    "# Create a dataframe for unigrams\n",
    "df_unigram = pd.DataFrame(count_unigram.toarray(),\n",
    "                          columns=vectorizer_unigram.get_feature_names_out())\n",
    "\n",
    "print('Document-term matrix (Unigrams)')\n",
    "print(df_unigram)\n",
    "\n",
    "# Document-term matrix with ngram_range=(2,2) [Bigrams]\n",
    "vectorizer_bigram = CountVectorizer(ngram_range=(2,2), min_df=0.0)\n",
    "\n",
    "# Transform the corpus\n",
    "count_bigram = vectorizer_bigram.fit_transform(corpus)\n",
    "\n",
    "# Create a dataframe for bigrams\n",
    "df_bigram = pd.DataFrame(count_bigram.toarray(),\n",
    "                         columns=vectorizer_bigram.get_feature_names_out())\n",
    "\n",
    "print('\\nDocument-term matrix (Bigrams)')\n",
    "print(df_bigram)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417feb3a",
   "metadata": {},
   "source": [
    "## Document-term matrix with ngram_range=(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4eb33ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document-term matrix\n",
      "   bird fly  cat sit  dog run  fly tree  run park  sit window\n",
      "0         0        1        0         0         0           1\n",
      "1         0        0        1         0         1           0\n",
      "2         1        0        0         1         0           0\n"
     ]
    }
   ],
   "source": [
    "# Vectorizer with with ngram_range=(2,2)\n",
    "vectorizer = CountVectorizer(min_df=0.0, ngram_range=(2,2))\n",
    "\n",
    "# Transform \n",
    "count = vectorizer.fit_transform(corpus)\n",
    " \n",
    "# Create dataframe\n",
    "df_count = pd.DataFrame(count.toarray(),\n",
    "                        columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "print('Document-term matrix')\n",
    "print(df_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c846a236",
   "metadata": {},
   "source": [
    "## Term frequency-inverse document frequency (TF-IDF)\n",
    "- For details see: https://www.learndatasci.com/glossary/tf-idf-term-frequency-inverse-document-frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5854fa81",
   "metadata": {},
   "source": [
    "### Term Frequency (TF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f9ff38f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the corpus: 9 \n",
      "\n",
      "The words in the corpus: \n",
      " {'dog', 'tree', 'window', 'park', 'bird', 'sit', 'run', 'fly', 'cat'}\n",
      "\n",
      "Term Frequency (TF):\n",
      "      dog    tree  window    park    bird     sit     run     fly     cat\n",
      "0  0.0000  0.0000  0.3333  0.0000  0.0000  0.3333  0.0000  0.0000  0.3333\n",
      "1  0.3333  0.0000  0.0000  0.3333  0.0000  0.0000  0.3333  0.0000  0.0000\n",
      "2  0.0000  0.3333  0.0000  0.0000  0.3333  0.0000  0.0000  0.3333  0.0000\n"
     ]
    }
   ],
   "source": [
    "# Compute Term Frequency (TF)\n",
    "words_set = set()\n",
    "for doc in corpus:\n",
    "    words = doc.split(' ')\n",
    "    words_set = words_set.union(set(words))\n",
    "    \n",
    "print('Number of words in the corpus:',len(words_set), '\\n')\n",
    "print('The words in the corpus: \\n', words_set)\n",
    "\n",
    "# Number of documents in the corpus\n",
    "n_docs = len(corpus)\n",
    "\n",
    "# Number of unique words in the corpus \n",
    "n_words_set = len(words_set)\n",
    "\n",
    "df_tf = pd.DataFrame(np.zeros((n_docs, n_words_set)), \n",
    "                     columns=list(words_set))\n",
    "\n",
    "print(\"\\nTerm Frequency (TF):\")\n",
    "for i in range(n_docs):\n",
    "    # Words in the document\n",
    "    words = corpus[i].split(' ')\n",
    "    for w in words:\n",
    "        df_tf[w][i] = df_tf[w][i] + (1 / len(words))\n",
    "        \n",
    "print(df_tf.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91dae3d",
   "metadata": {},
   "source": [
    "### Inverse Document Frequency (IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5fe31336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inverse Document Frequency (IDF):\n",
      "            dog:     0.4771\n",
      "           tree:     0.4771\n",
      "         window:     0.4771\n",
      "           park:     0.4771\n",
      "           bird:     0.4771\n",
      "            sit:     0.4771\n",
      "            run:     0.4771\n",
      "            fly:     0.4771\n",
      "            cat:     0.4771\n"
     ]
    }
   ],
   "source": [
    "# Computing Inverse Document Frequency (IDF)\n",
    "print(\"\\nInverse Document Frequency (IDF):\")\n",
    "\n",
    "idf = {}\n",
    "\n",
    "for w in words_set:\n",
    "    \n",
    "    # k = number of documents that contain this word\n",
    "    k = 0\n",
    "    \n",
    "    for i in range(n_docs):\n",
    "        if w in corpus[i].split():\n",
    "            k += 1\n",
    "            \n",
    "    idf[w] =  np.log10(n_docs / k).round(4)\n",
    "    \n",
    "    print(f'{w:>15}: {idf[w]:>10}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc493eae",
   "metadata": {},
   "source": [
    "### Term Frequency - Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9c5ae575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF:\n",
      "     dog   tree  window   park   bird    sit    run    fly    cat\n",
      "0  0.000  0.000   0.159  0.000  0.000  0.159  0.000  0.000  0.159\n",
      "1  0.159  0.000   0.000  0.159  0.000  0.000  0.159  0.000  0.000\n",
      "2  0.000  0.159   0.000  0.000  0.159  0.000  0.000  0.159  0.000\n"
     ]
    }
   ],
   "source": [
    "# Computing TF-IDF\n",
    "df_tf_idf = df_tf.copy()\n",
    "\n",
    "for w in words_set:\n",
    "    for i in range(n_docs):\n",
    "        df_tf_idf[w][i] = df_tf[w][i] * idf[w]\n",
    "\n",
    "print('\\nTF-IDF:')\n",
    "print(df_tf_idf.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7b0f38",
   "metadata": {},
   "source": [
    "## Part-of-Speach (POS) tagging\n",
    "For meaning of POS-tags see: https://pythonexamples.org/nltk-pos-tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5c8c05c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT', 'O'),\n",
      " ('United', 'NNP', 'O'),\n",
      " ('Nations', 'NNP', 'O'),\n",
      " ('warned', 'VBD', 'O'),\n",
      " ('that', 'IN', 'O'),\n",
      " ('climate', 'NN', 'B-NP'),\n",
      " ('change', 'NN', 'B-NP'),\n",
      " ('is', 'VBZ', 'O'),\n",
      " ('a', 'DT', 'B-NP'),\n",
      " ('severe', 'JJ', 'I-NP'),\n",
      " ('global', 'JJ', 'I-NP'),\n",
      " ('crisis', 'NN', 'I-NP'),\n",
      " ('.', '.', 'O'),\n",
      " ('Many', 'JJ', 'O'),\n",
      " ('countries', 'NNS', 'O'),\n",
      " ('are', 'VBP', 'O'),\n",
      " ('working', 'VBG', 'O'),\n",
      " ('together', 'RB', 'O'),\n",
      " ('to', 'TO', 'O'),\n",
      " ('reduce', 'VB', 'O'),\n",
      " ('carbon', 'NN', 'B-NP'),\n",
      " ('emissions', 'NNS', 'O'),\n",
      " ('and', 'CC', 'O'),\n",
      " ('protect', 'VBP', 'O'),\n",
      " ('the', 'DT', 'B-NP'),\n",
      " ('environment', 'NN', 'I-NP'),\n",
      " ('..', 'NN', 'B-NP')]\n"
     ]
    }
   ],
   "source": [
    "text = '''The United Nations warned that climate change is a severe global crisis. \n",
    "          Many countries are working together to reduce carbon emissions and protect the environment..'''\n",
    "\n",
    "def preprocess(sent):\n",
    "    sent = nltk.word_tokenize(sent)\n",
    "    sent = nltk.pos_tag(sent)\n",
    "    return sent\n",
    "\n",
    "sent = preprocess(text)\n",
    "pattern = 'NP: {<DT>?<JJ>*<NN>}'\n",
    "\n",
    "cp = nltk.RegexpParser(pattern)\n",
    "cs = cp.parse(sent)\n",
    "\n",
    "iob_tagged = tree2conlltags(cs)\n",
    "\n",
    "# Print the POS-tags\n",
    "pprint(iob_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d906d5a",
   "metadata": {},
   "source": [
    "- DT (Determiner):\n",
    "Examples: \"The\", \"a\", \"the\"\n",
    "- NN (Noun, singular or mass):\n",
    "Examples: \"climate\", \"change\", \"crisis\", \"environment\"\n",
    "- NNP (Proper noun, singular):\n",
    "Examples: \"United\", \"Nations\"\n",
    "- VBD (Verb, past tense):\n",
    "Example: \"warned\"\n",
    "- JJ (Adjective):\n",
    "Examples: \"severe\", \"global\", \"Many\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1243de",
   "metadata": {},
   "source": [
    "### Jupyter notebook --footer info-- (please always provide this at the end of each submitted notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "017357b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "POSIX\n",
      "Linux | 6.5.0-1025-azure\n",
      "Datetime: 2024-12-15 21:58:22\n",
      "Python Version: 3.11.10\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import platform\n",
    "import socket\n",
    "from platform import python_version\n",
    "from datetime import datetime\n",
    "\n",
    "print('-----------------------------------')\n",
    "print(os.name.upper())\n",
    "print(platform.system(), '|', platform.release())\n",
    "print('Datetime:', datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "print('Python Version:', python_version())\n",
    "print('-----------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
